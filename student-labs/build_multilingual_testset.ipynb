{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08366010-8733-40ef-aff3-41581d504afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from itertools import islice\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c852ede-1dd6-4ca0-9869-50d95232feec",
   "metadata": {},
   "source": [
    "langs = [\"eng_Latn\", \"bul_Cyrl\", \"kor_Hang\", \"jpn_Jpan\"]\n",
    "samples = {}\n",
    "\n",
    "def serialize_datetime(obj):\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "    \n",
    "for lang in langs:\n",
    "    print(f\"Loading samples from: {lang}\")\n",
    "    stream = load_dataset(\"HPLT/HPLT2.0_cleaned\", lang, split=\"train\", streaming=True)\n",
    "    samples[lang] = list(islice(stream, 1500))\n",
    "\n",
    "\n",
    "with open(\"test_data.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for lang, items in samples.items():\n",
    "        for item in items:\n",
    "            item[\"lang_group\"] = lang\n",
    "            f.write(json.dumps(item, ensure_ascii=False, default=serialize_datetime) + \"\\n\")\n",
    "\n",
    "print(\"test_data.jsonl saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b784947f-4a0c-46a1-a7c5-50788c3e47f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cc6c95d-e374-4e74-b306-5c29a87f7f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4000 filtered sentences\n",
      "eng_Latn: 1000\n",
      "bul_Cyrl: 1000\n",
      "kor_Hang: 1000\n",
      "jpn_Jpan: 1000\n"
     ]
    }
   ],
   "source": [
    "def extract_valid_sentence(text, min_len=20, max_len=200):\n",
    "    sentences = re.split(r'[.!?。？！]\\s*', text.strip())\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if min_len <= len(sentence) <= max_len:\n",
    "            return sentence\n",
    "    return None\n",
    "\n",
    "min_len = 20\n",
    "max_len = 200\n",
    "max_per_language = 1000\n",
    "\n",
    "with open(\"test_data.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = [json.loads(line) for line in f]\n",
    "\n",
    "seen_texts = set()\n",
    "language_counts = defaultdict(int)\n",
    "filtered_data = []\n",
    "\n",
    "for item in raw_data:\n",
    "    lang = item[\"lang_group\"]\n",
    "    \n",
    "    if language_counts[lang] >= max_per_language:\n",
    "        continue\n",
    "\n",
    "    sentence = extract_valid_sentence(item[\"text\"], min_len, max_len)\n",
    "    if sentence and sentence not in seen_texts:\n",
    "        seen_texts.add(sentence)\n",
    "        language_counts[lang] += 1\n",
    "        filtered_data.append({\"lang\": lang, \"text\": sentence})\n",
    "\n",
    "with open(\"test_data_cleaned.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in filtered_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Saved {len(filtered_data)} filtered sentences\")\n",
    "for lang, count in language_counts.items():\n",
    "    print(f\"{lang}: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
